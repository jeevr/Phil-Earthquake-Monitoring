{
	"name": "dataflow_segregate_by_day",
	"properties": {
		"type": "MappingDataFlow",
		"typeProperties": {
			"sources": [
				{
					"dataset": {
						"referenceName": "CSVInput",
						"type": "DatasetReference"
					},
					"name": "inputCSV"
				}
			],
			"sinks": [
				{
					"dataset": {
						"referenceName": "CSVOutput",
						"type": "DatasetReference"
					},
					"name": "sink1"
				}
			],
			"transformations": [
				{
					"name": "filterday01"
				}
			],
			"scriptLines": [
				"parameters{",
				"     df_param_year as string,",
				"     df_param_month as string,",
				"     df_param_day as string",
				"}",
				"source(output(",
				"          date_time as string,",
				"          date as date 'yyyy-MM-dd',",
				"          time as string,",
				"          latitude as double,",
				"          longitude as double,",
				"          depth_km as short,",
				"          depth_km_symbol as string,",
				"          magnitude as double,",
				"          location as string,",
				"          hlink as string,",
				"          details as string",
				"     ),",
				"     allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     limit: 100,",
				"     ignoreNoFilesFound: false) ~> inputCSV",
				"inputCSV filter(equals(date, toDate(concat($df_param_year, '-', lpad($df_param_month, 2, '0'), '-', lpad($df_param_day, 2, '0'))))) ~> filterday01",
				"filterday01 sink(allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     filePattern:(concat('earthquake_data_', lpad($df_param_day, 2, '0'))),",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true) ~> sink1"
			]
		}
	}
}